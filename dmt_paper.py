# -*- coding: utf-8 -*-
"""dmt paper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q8X1RotKT9Xm9Xwnd497Q2CYXXm-CT2B
"""

from google.colab import files


uploaded = files.upload()

# prompt: perform data cleaning ,removing of noise from the dataset,remove outliers

import pandas as pd
import numpy as np

# Assuming you uploaded a CSV file named 'data.csv'
df = pd.read_csv('solar radiation.csv')

# 1. Handling Missing Values
# Replace missing values with the mean, median, or mode based on the column type
# 1. Handling Missing Values
# Replace missing values with the mean, median, or mode based on the column type
for column in df.columns:
  if df[column].dtype == 'object':
    df[column].fillna(df[column].mode()[0], inplace=True) #fillna is used to fill NaN values
  else:
    df[column].fillna(df[column].mean(), inplace=True) #fillna is used to fill NaN values

# 2. Removing Noise (e.g., incorrect data types)
# Convert columns to appropriate data types
for column in df.columns:
  if df[column].dtype == 'object' and pd.to_numeric(df[column], errors='coerce').notnull().all():
    df[column] = pd.to_numeric(df[column])


# 3. Removing Outliers (using IQR method)
for column in df.select_dtypes(include=np.number).columns:
  Q1 = df[column].quantile(0.25)
  Q3 = df[column].quantile(0.75)
  IQR = Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR
  df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

# Print the cleaned DataFrame
print(df)

import pandas as pd

df = pd.read_csv('solar radiation.csv')

print(df.head(10))

import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.decomposition import PCA

# Load the dataframe (Assuming you have saved the cleaned dataframe as 'cleaned_data.csv')
df = pd.read_csv('solar radiation.csv') # Load the cleaned dataframe

# Extract numerical features
numerical_df = df.select_dtypes(include=np.number) # Select numerical features from the DataFrame

# Check for valid numerical features after outlier removal and NaN imputation
if len(numerical_df.columns) > 0 and numerical_df.shape[1] > 0:

    # Standardize the data
    x = StandardScaler().fit_transform(numerical_df)

    # Impute NaN values (though there should be none after previous steps)
    imputer = SimpleImputer(strategy='mean') # Replace NaN with the mean
    x = imputer.fit_transform(x)

    if x.shape[1] > 0:  # Ensure there are features after imputation
        # Apply PCA
        pca = PCA(n_components=0.95) # Keep components that explain 95% of the variance
        principalComponents = pca.fit_transform(x)

        # Create a new DataFrame with principal components
        principalDf = pd.DataFrame(data=principalComponents)

        # Print the explained variance ratio
        print("Explained Variance Ratio:", pca.explained_variance_ratio_)

        # Print the new DataFrame with principal components
        print(principalDf)

    else:
        print("No valid features left after imputation.")

else:
    print("No numerical features available for PCA.")

!pip install scikit-learn

# prompt: apply normalization to my dataset

import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# Assuming 'numerical_df' contains your numerical features

# Create a MinMaxScaler object
scaler = MinMaxScaler()

# Fit and transform the data
normalized_data = scaler.fit_transform(numerical_df)

# Create a new DataFrame with the normalized data
normalized_df = pd.DataFrame(normalized_data, columns=numerical_df.columns)

# Print the normalized DataFrame
print(normalized_df)

from sklearn.model_selection import train_test_split

# Assuming 'df' is your cleaned DataFrame and you want to predict 'Temperature'
# Check if 'Temperature' column exists before dropping
if 'Temperature' in df.columns:
  X = df.drop('Temperature', axis=1)  # Features
  y = df['Temperature']  # Target

  # Split the data into training and testing sets (e.g., 80% training, 20% testing)
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

  # Now you have X_train, X_test, y_train, and y_test for model training and evaluation
  print("Training set shape:", X_train.shape, y_train.shape)
  print("Testing set shape:", X_test.shape, y_test.shape)
else:
  print("Column 'Temperature' not found in DataFrame.")

# Print some info to check your DataFrame
print(df.info())
print(df.head())

df.info()

# prompt: apply svm model to my dataset

from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error

# Create an SVM model for regression
svm_model = SVR(kernel='linear')  # Example: Linear kernel

# Train the model on the training data
svm_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = svm_model.predict(X_test)

# Evaluate the model using a regression metric (e.g., Mean Squared Error)
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

# prompt: i want accuracy for svm

from sklearn.metrics import accuracy_score
from sklearn.svm import SVC # Import SVC for classification


# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create an SVM model for classification
svm_model_classification = SVC(kernel='linear')  # Example: Linear kernel

# Train the model on the training data
svm_model_classification.fit(X_train, y_train_categorical)

# Make predictions on the test data
y_pred_categorical = svm_model_classification.predict(X_test)

# Calculate the accuracy
accuracy = accuracy_score(y_test_categorical, y_pred_categorical)
print(f"Accuracy: {accuracy}")

df.isnull().sum()

df.info()

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Load your dataset
# df = pd.read_csv('your_dataset.csv')  # Uncomment and specify your dataset file

# Example: Assuming you have loaded the dataset into a DataFrame 'df'
# Specify the feature columns and the target column
X = df.drop('Temperature', axis=1)  # Replace 'target_column' with your actual target column
y = df['Temperature']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Gradient Boosting Regressor
gb_model = GradientBoostingRegressor()

# Train the model
gb_model.fit(X_train, y_train)

# Make predictions
y_pred = gb_model.predict(X_test)

# Calculate the performance metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R-squared: {r2:.2f}")

# prompt: i want accuracy for gradient boost

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create a Gradient Boosting model for classification
gb_model_classification = GradientBoostingClassifier()

# Train the model on the training data
gb_model_classification.fit(X_train, y_train_categorical)

# Make predictions on the test data
y_pred_categorical = gb_model_classification.predict(X_test)

# Calculate the accuracy
accuracy = accuracy_score(y_test_categorical, y_pred_categorical)
print(f"Accuracy: {accuracy}")

# prompt: apply decision tree model to my dataset and find accuracy

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create a Decision Tree model for classification
dt_model = DecisionTreeClassifier()

# Train the model on the training data
dt_model.fit(X_train, y_train_categorical)

# Make predictions on the test data
y_pred_categorical = dt_model.predict(X_test)

# Calculate the accuracy
accuracy = accuracy_score(y_test_categorical, y_pred_categorical)
print(f"Accuracy: {accuracy}")

# prompt: apply  XGBoost model to my dataset and find accuracy

!pip install xgboost

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create an XGBoost model for classification
xgb_model = XGBClassifier()

# Train the model on the training data
xgb_model.fit(X_train, y_train_categorical)

# Make predictions on the test data
y_pred_categorical = xgb_model.predict(X_test)

# Calculate the accuracy
accuracy = accuracy_score(y_test_categorical, y_pred_categorical)
print(f"Accuracy: {accuracy}")

# prompt: apply  linear regression model to my dataset and find accuracy

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Create a Linear Regression model
linear_model = LinearRegression()

# Train the model on the training data
linear_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = linear_model.predict(X_test)

# Evaluate the model using regression metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R-squared: {r2:.2f}")

# You can consider R-squared as a measure of accuracy for linear regression.
# Higher R-squared values indicate a better fit of the model to the data.

# prompt: i want accuracy for linear regrssion

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Create a Linear Regression model
linear_model = LinearRegression()

# Train the model on the training data
linear_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = linear_model.predict(X_test)

# Calculate the R-squared score, which can be considered as a measure of accuracy for linear regression.
r2 = r2_score(y_test, y_pred)

print(f"R-squared (Accuracy): {r2:.2f}")

# prompt: apply  K-Nearest Neighbours model to my dataset and find accuracy

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create a K-Nearest Neighbors model for classification
knn_model = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (n_neighbors)

# Train the model on the training data
knn_model.fit(X_train, y_train_categorical)

# Make predictions on the test data
y_pred_categorical = knn_model.predict(X_test)

# Calculate the accuracy
accuracy = accuracy_score(y_test_categorical, y_pred_categorical)
print(f"Accuracy: {accuracy}")

# prompt: apply  K-Nearest Neighbours and linear regrssion by hybriding these model and apply  to my dataset and find accuracy

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Create a Linear Regression model
linear_model = LinearRegression()

# Train the model on the training data
linear_model.fit(X_train, y_train)

# Make predictions using Linear Regression
y_pred_linear = linear_model.predict(X_test)

# Create a K-Nearest Neighbors model for classification
knn_model = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors (n_neighbors)

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)


# Train the KNN model on the training data
knn_model.fit(X_train, y_train_categorical)

# Make predictions using KNN
y_pred_knn = knn_model.predict(X_test)


# Hybrid approach: Combine predictions from both models
# Example: Average the predictions for regression tasks
y_pred_hybrid = (y_pred_linear + y_pred_knn) / 2  # Adjust as needed

# Calculate the accuracy for the hybrid approach
# Note: This depends on how you are combining the models
# Here, we're assuming you're doing regression and want to evaluate based on MSE
mse_hybrid = mean_squared_error(y_test, y_pred_hybrid)
print(f"Hybrid Model Mean Squared Error: {mse_hybrid}")

# For classification, use accuracy
accuracy_hybrid = accuracy_score(y_test_categorical, (y_pred_hybrid > threshold).astype(int))
print(f"Hybrid Model Accuracy: {accuracy_hybrid}")

# prompt: apply gradient boost and xg boost to my model by hybriding these two models and find accuracy

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create a Gradient Boosting model for classification
gb_model_classification = GradientBoostingClassifier()

# Train the model on the training data
gb_model_classification.fit(X_train, y_train_categorical)

# Make predictions using Gradient Boosting
y_pred_gb = gb_model_classification.predict(X_test)

# Create an XGBoost model for classification
xgb_model = XGBClassifier()

# Train the model on the training data
xgb_model.fit(X_train, y_train_categorical)

# Make predictions using XGBoost
y_pred_xgb = xgb_model.predict(X_test)

# Hybrid approach: Combine predictions from both models
# Example: Use a simple voting mechanism (majority vote)
y_pred_hybrid = [1 if (y_pred_gb[i] + y_pred_xgb[i]) >= 1 else 0 for i in range(len(y_pred_gb))]

# Calculate the accuracy for the hybrid approach
accuracy_hybrid = accuracy_score(y_test_categorical, y_pred_hybrid)
print(f"Hybrid (Gradient Boosting + XGBoost) Model Accuracy: {accuracy_hybrid}")

# prompt: hybride decision tree and gradient boost and find accuracy

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create a Decision Tree model for classification
dt_model = DecisionTreeClassifier()

# Train the model on the training data
dt_model.fit(X_train, y_train_categorical)

# Make predictions using Decision Tree
y_pred_dt = dt_model.predict(X_test)

# Create a Gradient Boosting model for classification
gb_model_classification = GradientBoostingClassifier()

# Train the model on the training data
gb_model_classification.fit(X_train, y_train_categorical)

# Make predictions using Gradient Boosting
y_pred_gb = gb_model_classification.predict(X_test)

# Hybrid approach: Combine predictions from both models
# Example: Use a simple voting mechanism (majority vote)
y_pred_hybrid = [1 if (y_pred_dt[i] + y_pred_gb[i]) >= 1 else 0 for i in range(len(y_pred_dt))]

# Calculate the accuracy for the hybrid approach
accuracy_hybrid = accuracy_score(y_test_categorical, y_pred_hybrid)
print(f"Hybrid (Decision Tree + Gradient Boosting) Model Accuracy: {accuracy_hybrid}")

# prompt: apply navies bayes model and find accuracy

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create a Naive Bayes model for classification
nb_model = GaussianNB()

# Train the model on the training data
nb_model.fit(X_train, y_train_categorical)

# Make predictions on the test data
y_pred_categorical = nb_model.predict(X_test)

# Calculate the accuracy
accuracy = accuracy_score(y_test_categorical, y_pred_categorical)
print(f"Naive Bayes Model Accuracy: {accuracy}")

# prompt: hybride navies bayes and grsdient boost and find accuracy

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create a Naive Bayes model for classification
nb_model = GaussianNB()

# Train the model on the training data
nb_model.fit(X_train, y_train_categorical)

# Make predictions using Naive Bayes
y_pred_nb = nb_model.predict(X_test)

# Create a Gradient Boosting model for classification
gb_model_classification = GradientBoostingClassifier()

# Train the model on the training data
gb_model_classification.fit(X_train, y_train_categorical)

# Make predictions using Gradient Boosting
y_pred_gb = gb_model_classification.predict(X_test)

# Hybrid approach: Combine predictions from both models
# Example: Use a simple voting mechanism (majority vote)
y_pred_hybrid = [1 if (y_pred_nb[i] + y_pred_gb[i]) >= 1 else 0 for i in range(len(y_pred_nb))]

# Calculate the accuracy for the hybrid approach
accuracy_hybrid = accuracy_score(y_test_categorical, y_pred_hybrid)
print(f"Hybrid (Naive Bayes + Gradient Boosting) Model Accuracy: {accuracy_hybrid}")

# prompt: apply random forest and find accuracy

from sklearn.ensemble import RandomForestClassifier

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create a Random Forest model for classification
rf_model = RandomForestClassifier()

# Train the model on the training data
rf_model.fit(X_train, y_train_categorical)

# Make predictions on the test data
y_pred_categorical = rf_model.predict(X_test)

# Calculate the accuracy
accuracy = accuracy_score(y_test_categorical, y_pred_categorical)
print(f"Random Forest Model Accuracy: {accuracy}")

# prompt: apply rigid regression and find accuracy

from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Create a Ridge Regression model
ridge_model = Ridge(alpha=1.0)  # You can adjust the regularization strength (alpha)

# Train the model on the training data
ridge_model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = ridge_model.predict(X_test)

# Evaluate the model using regression metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R-squared (Accuracy): {r2:.2f}")

# prompt: hybride rigid regression and random forest and find accuracy

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Create a Ridge Regression model
ridge_model = Ridge(alpha=1.0)  # You can adjust the regularization strength (alpha)

# Train the model on the training data
ridge_model.fit(X_train, y_train)

# Make predictions using Ridge Regression
y_pred_ridge = ridge_model.predict(X_test)

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create a Random Forest model for classification
rf_model = RandomForestClassifier()

# Train the model on the training data
rf_model.fit(X_train, y_train_categorical)

# Make predictions using Random Forest
y_pred_rf = rf_model.predict(X_test)

# Hybrid approach: Combine predictions from both models
# Example: Use a simple voting mechanism (majority vote) for classification
y_pred_hybrid = [1 if (y_pred_ridge[i] > threshold and y_pred_rf[i] == 1) else 0 for i in range(len(y_pred_ridge))]


# Calculate the accuracy for the hybrid approach
accuracy_hybrid = accuracy_score(y_test_categorical, y_pred_hybrid)
print(f"Hybrid (Ridge Regression + Random Forest) Model Accuracy: {accuracy_hybrid}")

# prompt: apply Artificial Neural Network to my modeland find accuracy

import tensorflow as tf
from tensorflow import keras
from sklearn.metrics import accuracy_score

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Define the ANN model
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train_categorical, epochs=10, batch_size=32)

# Make predictions on the test data
y_pred_prob = model.predict(X_test)
y_pred_categorical = (y_pred_prob > 0.5).astype(int)

# Calculate the accuracy
accuracy = accuracy_score(y_test_categorical, y_pred_categorical)
print(f"ANN Model Accuracy: {accuracy}")

# prompt: apply gradient boosting regression tree  to my model and find accuracy

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Initialize the Gradient Boosting Regressor
gb_model = GradientBoostingRegressor()

# Train the model
gb_model.fit(X_train, y_train)

# Make predictions
y_pred = gb_model.predict(X_test)

# Calculate the performance metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R-squared: {r2:.2f}")

# R-squared can be considered as a measure of accuracy for regression models.
# Higher R-squared values indicate a better fit of the model to the data.

# prompt: apply hybride of  gradient boosting regression tree and light gnm and find accuracy

!pip install lightgbm

from lightgbm import LGBMRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Create a Gradient Boosting Regressor model
gb_model = GradientBoostingRegressor()

# Train the Gradient Boosting model
gb_model.fit(X_train, y_train)

# Make predictions using Gradient Boosting
y_pred_gb = gb_model.predict(X_test)

# Create a LightGBM Regressor model
lgbm_model = LGBMRegressor()

# Train the LightGBM model
lgbm_model.fit(X_train, y_train)

# Make predictions using LightGBM
y_pred_lgbm = lgbm_model.predict(X_test)

# Hybrid approach: Average the predictions from both models
y_pred_hybrid = (y_pred_gb + y_pred_lgbm) / 2

# Calculate the performance metrics for the hybrid model
mse_hybrid = mean_squared_error(y_test, y_pred_hybrid)
r2_hybrid = r2_score(y_test, y_pred_hybrid)

print(f"Hybrid (Gradient Boosting + LightGBM) Model Mean Squared Error: {mse_hybrid:.2f}")
print(f"Hybrid (Gradient Boosting + LightGBM) Model R-squared: {r2_hybrid:.2f}")

# R-squared can be considered as a measure of accuracy for regression models.
# Higher R-squared values indicate a better fit of the model to the data.

# prompt:  apply hybride of  gradient boosting regression tree and light gnm and apply Artificial Neural Network  find accuracy

from lightgbm import LGBMRegressor

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Create a Gradient Boosting Regressor model
gb_model = GradientBoostingRegressor()

# Train the Gradient Boosting model
gb_model.fit(X_train, y_train)

# Make predictions using Gradient Boosting
y_pred_gb = gb_model.predict(X_test)

# Create a LightGBM Regressor model
lgbm_model = LGBMRegressor()

# Train the LightGBM model
lgbm_model.fit(X_train, y_train)

# Make predictions using LightGBM
y_pred_lgbm = lgbm_model.predict(X_test)

# Hybrid approach: Average the predictions from both models
y_pred_hybrid = (y_pred_gb + y_pred_lgbm) / 2

# Calculate the performance metrics for the hybrid model
mse_hybrid = mean_squared_error(y_test, y_pred_hybrid)
r2_hybrid = r2_score(y_test, y_pred_hybrid)

print(f"Hybrid (Gradient Boosting + LightGBM) Model Mean Squared Error: {mse_hybrid:.2f}")
print(f"Hybrid (Gradient Boosting + LightGBM) Model R-squared: {r2_hybrid:.2f}")


# ANN Model
# Assuming you have X_train, X_test, y_train, y_test from previous code

# Define the ANN model
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1)  # Output layer for regression
])

# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])

# Train the model
model.fit(X_train, y_train, epochs=10, batch_size=32)

# Make predictions on the test data
y_pred_ann = model.predict(X_test)

# Calculate the performance metrics for ANN
mse_ann = mean_squared_error(y_test, y_pred_ann)
r2_ann = r2_score(y_test, y_pred_ann)

print(f"ANN Model Mean Squared Error: {mse_ann:.2f}")
print(f"ANN Model R-squared: {r2_ann:.2f}")

# Hybrid approach: Combine predictions from Hybrid and ANN
y_pred_final = (y_pred_hybrid + y_pred_ann.flatten()) / 2

# Calculate the performance metrics for the final hybrid model
mse_final = mean_squared_error(y_test, y_pred_final)
r2_final = r2_score(y_test, y_pred_final)

print(f"Final Hybrid (Gradient Boosting + LightGBM + ANN) Model Mean Squared Error: {mse_final:.2f}")
print(f"Final Hybrid (Gradient Boosting + LightGBM + ANN) Model R-squared: {r2_final:.2f}")

# prompt: i want accuracy for the above hybrid model

# Assuming you have y_test and y_pred_final from the previous code

# Convert the continuous predictions to categorical predictions
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_test_categorical = (y_test > threshold).astype(int)
y_pred_final_categorical = (y_pred_final > threshold).astype(int)

# Calculate the accuracy of the final hybrid model
accuracy_final = accuracy_score(y_test_categorical, y_pred_final_categorical)
print(f"Final Hybrid Model Accuracy: {accuracy_final}")

# prompt: apply Back Propagation Neural Network to my model and find accuracy

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Define the Backpropagation Neural Network model
model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])

# Compile the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train_categorical, epochs=10, batch_size=32)

# Evaluate the model on the test data
loss, accuracy = model.evaluate(X_test, y_test_categorical)
print(f"Backpropagation Neural Network Model Accuracy: {accuracy}")

# prompt: apply cat boost to my model and find accuracy

# Install catboost if not already installed
!pip install catboost

from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create a CatBoost model for classification
catboost_model = CatBoostClassifier(iterations=100, # You can adjust the number of iterations
                                    learning_rate=0.1, # Adjust learning rate
                                    depth=6, # Adjust tree depth
                                    verbose=False) # Set to True to see training progress

# Train the model on the training data
catboost_model.fit(X_train, y_train_categorical)

# Make predictions on the test data
y_pred_categorical = catboost_model.predict(X_test)

# Calculate the accuracy
accuracy = accuracy_score(y_test_categorical, y_pred_categorical)
print(f"CatBoost Model Accuracy: {accuracy}")

# prompt: apply LightGBM to my model and find accuracy

# Install LightGBM if not already installed
!pip install lightgbm

from lightgbm import LGBMClassifier

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create a LightGBM model for classification
lgbm_model = LGBMClassifier()

# Train the model on the training data
lgbm_model.fit(X_train, y_train_categorical)

# Make predictions on the test data
y_pred_categorical = lgbm_model.predict(X_test)

# Calculate the accuracy
accuracy = accuracy_score(y_test_categorical, y_pred_categorical)
print(f"LightGBM Model Accuracy: {accuracy}")

# prompt: hybride LightGBM and cat boost model

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create a LightGBM model for classification
lgbm_model = LGBMClassifier()

# Train the LightGBM model on the training data
lgbm_model.fit(X_train, y_train_categorical)

# Make predictions using LightGBM
y_pred_lgbm = lgbm_model.predict(X_test)

# Create a CatBoost model for classification
catboost_model = CatBoostClassifier(iterations=100, learning_rate=0.1, depth=6, verbose=False)

# Train the CatBoost model on the training data
catboost_model.fit(X_train, y_train_categorical)

# Make predictions using CatBoost
y_pred_catboost = catboost_model.predict(X_test)

# Hybrid approach: Combine predictions from both models
# Example: Use a simple voting mechanism (majority vote)
y_pred_hybrid = [1 if (y_pred_lgbm[i] + y_pred_catboost[i]) >= 1 else 0 for i in range(len(y_pred_lgbm))]

# Calculate the accuracy for the hybrid approach
accuracy_hybrid = accuracy_score(y_test_categorical, y_pred_hybrid)
print(f"Hybrid (LightGBM + CatBoost) Model Accuracy: {accuracy_hybrid}")

# prompt: hybride svm and LightGBM model and find accuracy

from lightgbm import LGBMClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)


# Create a LightGBM model for classification
lgbm_model = LGBMClassifier()

# Train the LightGBM model on the training data
lgbm_model.fit(X_train, y_train_categorical)

# Make predictions using LightGBM
y_pred_lgbm = lgbm_model.predict(X_test)


# Create an SVM model for classification
svm_model = SVC(probability=True)  # Use probability=True for soft voting

# Train the SVM model on the training data
svm_model.fit(X_train, y_train_categorical)

# Make predictions using SVM
y_pred_svm = svm_model.predict(X_test)


# Hybrid approach: Combine predictions from both models
# Example: Use a simple voting mechanism (majority vote)
y_pred_hybrid = [1 if (y_pred_lgbm[i] + y_pred_svm[i]) >= 1 else 0 for i in range(len(y_pred_lgbm))]

# Calculate the accuracy for the hybrid approach
accuracy_hybrid = accuracy_score(y_test_categorical, y_pred_hybrid)
print(f"Hybrid (LightGBM + SVM) Model Accuracy: {accuracy_hybrid}")

# prompt: apply AdaBoost

from sklearn.ensemble import AdaBoostClassifier

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create an AdaBoost model for classification
ada_model = AdaBoostClassifier()

# Train the model on the training data
ada_model.fit(X_train, y_train_categorical)

# Make predictions on the test data
y_pred_categorical = ada_model.predict(X_test)

# Calculate the accuracy
accuracy = accuracy_score(y_test_categorical, y_pred_categorical)
print(f"AdaBoost Model Accuracy: {accuracy}")

# prompt: hybrid AdaBoost  and light gvm

from sklearn.ensemble import AdaBoostClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create a LightGBM model for classification
lgbm_model = LGBMClassifier()

# Train the LightGBM model on the training data
lgbm_model.fit(X_train, y_train_categorical)

# Make predictions using LightGBM
y_pred_lgbm = lgbm_model.predict(X_test)


# Create an AdaBoost model for classification
ada_model = AdaBoostClassifier()

# Train the AdaBoost model on the training data
ada_model.fit(X_train, y_train_categorical)

# Make predictions using AdaBoost
y_pred_ada = ada_model.predict(X_test)


# Hybrid approach: Combine predictions from both models
# Example: Use a simple voting mechanism (majority vote)
y_pred_hybrid = [1 if (y_pred_lgbm[i] + y_pred_ada[i]) >= 1 else 0 for i in range(len(y_pred_lgbm))]

# Calculate the accuracy for the hybrid approach
accuracy_hybrid = accuracy_score(y_test_categorical, y_pred_hybrid)
print(f"Hybrid (LightGBM + AdaBoost) Model Accuracy: {accuracy_hybrid}")

# prompt: apply lasso ,logistic and ridge regressions models do hybrid of these models  to my dataset and find accuracy

from sklearn.linear_model import Lasso, LogisticRegression, Ridge
from sklearn.metrics import accuracy_score

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)


# Create a Lasso Regression model
lasso_model = Lasso(alpha=0.1)  # You can adjust the regularization strength (alpha)

# Train the Lasso model on the training data
lasso_model.fit(X_train, y_train_categorical)

# Make predictions using Lasso Regression
y_pred_lasso = lasso_model.predict(X_test)
y_pred_lasso_categorical = (y_pred_lasso > 0.5).astype(int)


# Create a Logistic Regression model
logistic_model = LogisticRegression(max_iter=1000)

# Train the Logistic Regression model on the training data
logistic_model.fit(X_train, y_train_categorical)

# Make predictions using Logistic Regression
y_pred_logistic = logistic_model.predict(X_test)



# Create a Ridge Regression model
ridge_model = Ridge(alpha=1.0)  # You can adjust the regularization strength (alpha)

# Train the Ridge model on the training data
ridge_model.fit(X_train, y_train_categorical)

# Make predictions using Ridge Regression
y_pred_ridge = ridge_model.predict(X_test)
y_pred_ridge_categorical = (y_pred_ridge > 0.5).astype(int)



# Hybrid approach: Combine predictions from all three models
# Example: Use a simple voting mechanism (majority vote)
y_pred_hybrid = [1 if (y_pred_lasso_categorical[i] + y_pred_logistic[i] + y_pred_ridge_categorical[i]) >= 2 else 0 for i in
                 range(len(y_pred_lasso_categorical))]


# Calculate the accuracy for the hybrid approach
accuracy_hybrid = accuracy_score(y_test_categorical, y_pred_hybrid)
print(f"Hybrid (Lasso + Logistic + Ridge) Model Accuracy: {accuracy_hybrid}")

# prompt: apply RBF Network model to my dataset and find accuracy

from sklearn.neural_network import MLPRegressor
from sklearn.metrics import accuracy_score

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Create an RBF Network model (using MLPRegressor with 'relu' activation)
rbf_model = MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000)

# Train the RBF Network model on the training data
rbf_model.fit(X_train, y_train_categorical)

# Make predictions on the test data
y_pred_rbf = rbf_model.predict(X_test)
y_pred_rbf_categorical = (y_pred_rbf > 0.5).astype(int)


# Calculate the accuracy
accuracy = accuracy_score(y_test_categorical, y_pred_rbf_categorical)
print(f"RBF Network Model Accuracy: {accuracy}")

# prompt: hybrid bpnn and rbfn models and find accjuracy

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Backpropagation Neural Network (BPNN)
bpnn_model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])
bpnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
bpnn_model.fit(X_train, y_train_categorical, epochs=10, batch_size=32)

# RBF Network (RBFN) - Using MLPRegressor with 'relu' activation
rbfn_model = MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000)
rbfn_model.fit(X_train, y_train_categorical)

# Make predictions using both models
y_pred_bpnn_prob = bpnn_model.predict(X_test)
y_pred_bpnn_categorical = (y_pred_bpnn_prob > 0.5).astype(int)

y_pred_rbfn = rbfn_model.predict(X_test)
y_pred_rbfn_categorical = (y_pred_rbfn > 0.5).astype(int)

# Hybrid approach: Combine predictions from both models (e.g., majority voting)
y_pred_hybrid = [1 if (y_pred_bpnn_categorical[i] + y_pred_rbfn_categorical[i]) >= 1 else 0 for i in range(len(y_pred_bpnn_categorical))]

# Calculate the accuracy for the hybrid approach
accuracy_hybrid = accuracy_score(y_test_categorical, y_pred_hybrid)
print(f"Hybrid (BPNN + RBFN) Model Accuracy: {accuracy_hybrid}")

# prompt: perform hybrid of bpnn ,rbfn,gradient boost

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Backpropagation Neural Network (BPNN)
bpnn_model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])
bpnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
bpnn_model.fit(X_train, y_train_categorical, epochs=10, batch_size=32)

# RBF Network (RBFN) - Using MLPRegressor with 'relu' activation
rbfn_model = MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000)
rbfn_model.fit(X_train, y_train_categorical)

# Gradient Boosting Classifier
gb_model = GradientBoostingClassifier()
gb_model.fit(X_train, y_train_categorical)


# Make predictions using all three models
y_pred_bpnn_prob = bpnn_model.predict(X_test)
y_pred_bpnn_categorical = (y_pred_bpnn_prob > 0.5).astype(int)

y_pred_rbfn = rbfn_model.predict(X_test)
y_pred_rbfn_categorical = (y_pred_rbfn > 0.5).astype(int)

y_pred_gb = gb_model.predict(X_test)


# Hybrid approach: Combine predictions from all three models (e.g., majority voting)
y_pred_hybrid = [1 if (y_pred_bpnn_categorical[i] + y_pred_rbfn_categorical[i] + y_pred_gb[i]) >= 2 else 0 for i in range(len(y_pred_bpnn_categorical))]

# Calculate the accuracy for the hybrid approach
accuracy_hybrid = accuracy_score(y_test_categorical, y_pred_hybrid)
print(f"Hybrid (BPNN + RBFN + Gradient Boosting) Model Accuracy: {accuracy_hybrid}")

# prompt: hybrid of rbfn and light bgm

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# RBF Network (RBFN) - Using MLPRegressor with 'relu' activation
rbfn_model = MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000)
rbfn_model.fit(X_train, y_train_categorical)

# Create a LightGBM model for classification
lgbm_model = LGBMClassifier()

# Train the LightGBM model on the training data
lgbm_model.fit(X_train, y_train_categorical)

# Make predictions using both models
y_pred_rbfn = rbfn_model.predict(X_test)
y_pred_rbfn_categorical = (y_pred_rbfn > 0.5).astype(int)

y_pred_lgbm = lgbm_model.predict(X_test)

# Hybrid approach: Combine predictions from both models (e.g., majority voting)
y_pred_hybrid = [1 if (y_pred_rbfn_categorical[i] + y_pred_lgbm[i]) >= 1 else 0 for i in range(len(y_pred_rbfn_categorical))]

# Calculate the accuracy for the hybrid approach
accuracy_hybrid = accuracy_score(y_test_categorical, y_pred_hybrid)
print(f"Hybrid (RBFN + LightGBM) Model Accuracy: {accuracy_hybrid}")

# prompt: hybride of bpnn and light gbm

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Backpropagation Neural Network (BPNN)
bpnn_model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])
bpnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
bpnn_model.fit(X_train, y_train_categorical, epochs=10, batch_size=32)

# Create a LightGBM model for classification
lgbm_model = LGBMClassifier()

# Train the LightGBM model on the training data
lgbm_model.fit(X_train, y_train_categorical)

# Make predictions using both models
y_pred_bpnn_prob = bpnn_model.predict(X_test)
y_pred_bpnn_categorical = (y_pred_bpnn_prob > 0.5).astype(int)

y_pred_lgbm = lgbm_model.predict(X_test)

# Hybrid approach: Combine predictions from both models (e.g., majority voting)
y_pred_hybrid = [1 if (y_pred_bpnn_categorical[i] + y_pred_lgbm[i]) >= 1 else 0 for i in range(len(y_pred_bpnn_categorical))]

# Calculate the accuracy for the hybrid approach
accuracy_hybrid = accuracy_score(y_test_categorical, y_pred_hybrid)
print(f"Hybrid (BPNN + LightGBM) Model Accuracy: {accuracy_hybrid}")

# prompt: hybrid bpnn rgbf and light bgm

# Assuming you have X_train, X_test, y_train, y_test from previous code

# Convert y_train and y_test to categorical if needed
# For example, if you want to predict whether the temperature is above or below a certain threshold:
threshold = 25  # Example threshold
y_train_categorical = (y_train > threshold).astype(int)
y_test_categorical = (y_test > threshold).astype(int)

# Backpropagation Neural Network (BPNN)
bpnn_model = keras.Sequential([
    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid')  # Output layer for binary classification
])
bpnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
bpnn_model.fit(X_train, y_train_categorical, epochs=10, batch_size=32)

# RBF Network (RBFN) - Using MLPRegressor with 'relu' activation
rbfn_model = MLPRegressor(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=1000)
rbfn_model.fit(X_train, y_train_categorical)

# Create a LightGBM model for classification
lgbm_model = LGBMClassifier()

# Train the LightGBM model on the training data
lgbm_model.fit(X_train, y_train_categorical)

# Make predictions using all three models
y_pred_bpnn_prob = bpnn_model.predict(X_test)
y_pred_bpnn_categorical = (y_pred_bpnn_prob > 0.5).astype(int)

y_pred_rbfn = rbfn_model.predict(X_test)
y_pred_rbfn_categorical = (y_pred_rbfn > 0.5).astype(int)

y_pred_lgbm = lgbm_model.predict(X_test)

# Hybrid approach: Combine predictions from all three models (e.g., majority voting)
y_pred_hybrid = [1 if (y_pred_bpnn_categorical[i] + y_pred_rbfn_categorical[i] + y_pred_lgbm[i]) >= 2 else 0 for i in range(len(y_pred_bpnn_categorical))]

# Calculate the accuracy for the hybrid approach
accuracy_hybrid = accuracy_score(y_test_categorical, y_pred_hybrid)
print(f"Hybrid (BPNN + RBFN + LightGBM) Model Accuracy: {accuracy_hybrid}")

import pandas as pd

# Data: Algorithm names and their accuracies
data = {
    "Algorithm Name": [
        "Hybrid (RBFN + LightGBM)",
        "Hybrid (BPNN + RBFN + LightGBM)",
        "Hybrid (BPNN + LightGBM)",
        "Hybrid (RBFN + LightGBM)",
        "Hybrid (BPNN + RBFN + Gradient Boosting)",
        "Hybrid (BPNN + RBFN)",
        "RBF Network",
        "Hybrid (Lasso + Logistic + Ridge)",
        "Hybrid (LightGBM + AdaBoost)",
        "AdaBoost",
        "Hybrid (LightGBM + SVM)",
        "Hybrid (LightGBM + CatBoost)",
        "LightGBM",
        "CatBoost",
        "Backpropagation Neural Network",
        "Final Hybrid (Gradient Boosting + LightGBM + ANN)",
        "Hybrid (Gradient Boosting + LightGBM) R-squared",
        "Gradient Boosting Regression Tree",
        "ANN",
        "Hybrid (Ridge Regression + Random Forest)",
        "Ridge Regression",
        "Random Forest",
        "Hybrid (Naive Bayes + Gradient Boosting)",
        "Naive Bayes",
        "Hybrid (Decision Tree + Gradient Boosting)",
        "Hybrid (Gradient Boosting + XGBoost)",
        "Hybrid (K-Nearest Neighbours + Linear Regression)",
        "K-Nearest Neighbours",
        "XGBoost",
        "Decision Tree",
        "Gradient Boosting",
        "SVM"
    ],
    "Accuracy": [
        0.8584,
        0.9555,
        0.8693,
        0.8584,
        0.9452,
        0.8288,
        0.8134,
        0.9481,
        0.9812,
        0.9709,
        0.8550,
        0.9857,
        0.9903,
        0.9817,
        0.9218,
        0.9344,
        0.9900,  # Assuming accuracy instead of R-squared here
        0.9800,
        0.8836,
        0.9549,
        0.9400,
        0.9760,
        0.9606,
        0.6969,
        0.9715,
        0.9817,
        0.2660,
        0.9053,
        0.9874,
        0.9646,
        0.9783,
        0.9692
    ]
}

# Create a DataFrame from the data
df = pd.DataFrame(data)

# Function to apply highlighting
def highlight_rows(row):
    # Highlight the specified rows
    if row['Algorithm Name'] in [
        "Hybrid (BPNN + RBFN + LightGBM)",
        "Hybrid (BPNN + RBFN + Gradient Boosting)"
    ]:
        return ['background-color: yellow'] * len(row)
    else:
        return [''] * len(row)

# Apply the highlight function
df_style = df.style.apply(highlight_rows, axis=1)

# Display the DataFrame with highlights
df_style